# MCQ Answer Evaluation Pipeline (LongHealth Example)

## Project Goal

This repository, part of a larger HTA Agent Orchestration study, implements and evaluates two distinct strategies for filtering and evaluating multiple-choice question (MCQ) answers generated by Large Language Models (LLMs), using the LongHealth benchmark dataset as a testbed.

The two workflows allow for comparing different evaluation paradigms:

1.  **Workflow A (Semantic Filter + LLM Judge):** Focuses on filtering generated answers based on meaning and then performing a detailed quality assessment using an external LLM Judge against HTA-inspired rubrics.
2.  **Workflow B (Log Probability Filter):** Explores using a "verifier" LLM's internal likelihood scores (log probabilities) to automatically select the best answer from a pool of generated candidates and measures the accuracy of this selection method.

## Pipeline Overview

Both workflows share common initial steps:

* **Step 0 (`scripts/0_prepare_data.sh`):** Prepares input data (LongHealth subset or dummy data) into `data/processed/longhealth_cleaned.jsonl`.
* **Step 1 (`scripts/1_generate_responses.sh`):** Generates candidate text answers using specified LLMs, saving results to `outputs/model_responses/`.

Then, the workflows diverge:

* **Workflow A (Semantic Filter + LLM Judge):**
    * **Step 2a (`scripts/2a_filter_semantic.sh`):** Filters generated answers from Step 1 using semantic embeddings, saving selected responses to `outputs/filtered_responses.jsonl`.
    * **Step 2b (`scripts/2b_evaluate_llm_judge.sh`):** Evaluates the filtered responses from Step 2a using an LLM Judge based on HTA metrics, saving scores to `outputs/evaluation_results.json`.

* **Workflow B (Log Probability Filter):**
    * **Step 2c (`scripts/2c_evaluate_logprob_filter.sh`):** Uses a specified "verifier" LLM to score *all* generated answers from Step 1 based on log probability (AvgNLL). Selects the highest-likelihood answer per query and compares it to ground truth, saving an evaluation summary (including Top-1 and Pass@k accuracy) to `outputs/logprob_evaluation/`. Can run in different modes (fixed verifier or fixed generator) configured within the script.
    * **Step 3a (`scripts/3a_analyze_logprob_agreement.sh`):** (Optional) Analyzes the agreement between different verifiers when Step 2c is run in "fixed generator" mode, saving results to `outputs/verifier_agreement_summary.json`.

## Setup

1.  **Clone Repository:**
    ```bash
    # git clone <your-repo-url>
    # cd <your-repo-name> # Navigate to the repo root (e.g., logprob_filter_repo)
    ```
2.  **Create Environment & Install Dependencies:**
    ```bash
    python -m venv venv
    source venv/bin/activate # or .\venv\Scripts\activate on Windows
    pip install -r requirements.txt
    # Optional installs:
    # pip install bitsandbytes # If using quantization + CUDA
    # pip install sentence-transformers # If running Workflow A (Step 2a)
    # pip install openai # If running Workflow A (Step 2b)
    ```
3.  **API Keys & Access:**
    * Copy `.env.example` to `.env`.
    * Add your `HUGGINGFACE_TOKEN`.
    * Add `OPENAI_API_KEY` if running Workflow A.
    * Log in: `huggingface-cli login`.
    * Ensure access for any **gated models** used.

## Usage

Navigate to folder `cd examples/longhealth/`
Make scripts executable: `chmod +x scripts/*.sh`

**Running Workflow A (Semantic Filter + LLM Judge):**

1.  Configure `config/models.yaml` for the desired LLM Judge (if not using default).
2.  Configure generator models in `scripts/1_generate_responses.sh`.
3.  Run the workflow:
    ```bash
    bash scripts/run_workflow_A_semantic.sh
    # You can pass --use-dummy to run_workflow_A_semantic.sh to use dummy data
    ```
4.  Check outputs in `outputs/evaluation_results.json`.

**Running Workflow B (Log Probability Filter):**

1.  Configure generator models and `NUM_RESPONSES` (>=3 recommended for Pass@3) in `scripts/1_generate_responses.sh`.
2.  Configure `scripts/2c_evaluate_logprob_filter.sh`:
    * Set `EVALUATION_SETTING` (`fixed_verifier` or `fixed_generator`).
    * Set the corresponding model list (`FIXED_VERIFIER` or `VARYING_VERIFIERS`). **Use accessible models!**
    * Set `FIXED_GENERATOR_RESP_FILE` if using that setting.
    * Set `PASS_K` (default 3).
    * Configure quantization if desired.
3.  Run the workflow:
    ```bash
    bash scripts/run_workflow_B_logprob.sh
    # You can pass --use-dummy to run_workflow_B_logprob.sh
    ```
    * It will prompt whether to run the agreement analysis (Step 3a) at the end if applicable.
4.  Check outputs in `outputs/logprob_evaluation/` and potentially `outputs/verifier_agreement_summary.json`.

**Running Individual Steps:** You can also run individual scripts (e.g., `./scripts/0_prepare_data.sh`, `./scripts/1_generate_responses.sh`, `./scripts/2c_evaluate_logprob_filter.sh`) after ensuring the necessary preceding steps have been completed.

## Testing Specific Evaluation Scenarios (Workflow B)

Here's how to test the specific features implemented in Workflow B:

**1. Testing Pass@k Accuracy (Example: Pass@3)**

* **Goal:** Verify the calculation of Pass@k accuracy (checking if the correct answer is within the top K ranked generated responses).
* **Steps:**
    1.  Edit `scripts/1_generate_responses.sh`: Set `NUM_RESPONSES=3` (or higher). Ensure at least one generator model is listed.
    2.  Run Step 1: `bash scripts/1_generate_responses.sh`
    3.  Edit `scripts/2c_evaluate_logprob_filter.sh`:
        * Set `PASS_K=3`.
        * Choose either `EVALUATION_SETTING`.
        * Configure the relevant verifier model(s). Use models you have access to.
    4.  Run Step 2c: `bash scripts/2c_evaluate_logprob_filter.sh`
    5.  **Verify:** Check the terminal output for "Pass@5 Accuracy: X.XX%" and the output JSON file(s) in `outputs/logprob_evaluation/` for the `"accuracy_percentage_topK"` field.

**2. Testing Setting 1: Fixed Verifier / Varying Generators**

* **Goal:** Evaluate how well a single verifier scores answers generated by different models.
* **Steps:**
    1.  Edit `scripts/1_generate_responses.sh`: Ensure the `MODELS` array lists the *generator* models you want to include (e.g., Qwen 1.5B, 3B, 7B; TinyLlama 1.1B). Set `NUM_RESPONSES` (e.g., 3).
    2.  Run Step 1: `bash scripts/1_generate_responses.sh`
    3.  Edit `scripts/2c_evaluate_logprob_filter.sh`:
        * Set `EVALUATION_SETTING="fixed_verifier"`.
        * Set `FIXED_VERIFIER` to *one* accessible model (e.g., `"tiiuae/falcon-7b-instruct"`). **Avoid inaccessible Llama models unless access is confirmed.**
    4.  Run Step 2c: `bash scripts/2c_evaluate_logprob_filter.sh`
    5.  **Verify:** Check the output JSON file (`outputs/logprob_evaluation/eval_FIXED_VERIFIER_*.json`). The `"response_files_used"` field confirms which generator outputs were scored. The accuracy reflects the chosen verifier's performance across that mix of generated answers.

**3. Testing Setting 2: Fixed Generator / Varying Verifiers & Agreement**

* **Goal:** Compare how different verifiers score answers from a single generator and analyze their agreement.
* **Steps:**
    1.  Edit `scripts/1_generate_responses.sh`: Ensure the `MODELS` array contains the *single* generator model you want (e.g., `"Qwen/Qwen2.5-7B-Instruct"`). Set `NUM_RESPONSES` (e.g., 3).
    2.  Run Step 1: `bash scripts/1_generate_responses.sh`
    3.  Edit `scripts/2c_evaluate_logprob_filter.sh`:
        * Set `EVALUATION_SETTING="fixed_generator"`.
        * Set `FIXED_GENERATOR_RESP_FILE` to the path of the response file generated in step 2 (e.g., `"./outputs/model_responses/Qwen_Qwen2.5-7B-Instruct_responses.jsonl"`).
        * Set `VARYING_VERIFIERS` to a list of accessible verifier models (e.g., `"tiiuae/falcon-7b-instruct"`, `"Qwen/Qwen1.5-1.8B-Chat"`, `"mistralai/Mistral-7B-Instruct-v0.3"`). **Avoid inaccessible Llama models unless access is confirmed.**
    4.  Run Step 2c: `bash scripts/2c_evaluate_logprob_filter.sh`. This creates multiple output files (one per verifier).
    5.  Run Step 3a: `bash scripts/3a_analyze_logprob_agreement.sh`
    6.  **Verify:** Check the individual `eval_FIXED_GEN_*_verifier_*.json` files for each verifier's accuracy. Check `outputs/verifier_agreement_summary.json` for agreement statistics (e.g., `full_agreement_rate`).
